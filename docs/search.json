[
  {
    "objectID": "data/sfs/Kathmandu/Kathmandu.html",
    "href": "data/sfs/Kathmandu/Kathmandu.html",
    "title": "MUSA 550",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  All_Merged_Districts_Ward_Wi2  ENG dataset\n\nAll_Merged_Districts_Ward_Wi2\n\n                 0 0     false"
  },
  {
    "objectID": "data/sfs/Bhaktapur/Bhaktapur.html",
    "href": "data/sfs/Bhaktapur/Bhaktapur.html",
    "title": "MUSA 550",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  All_Merged_Districts_Ward_Wi2  ENG dataset\n\nAll_Merged_Districts_Ward_Wi2\n\n                 0 0     false"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Unpacking Urban Data in Kathmandu",
    "section": "",
    "text": "This is the project website.\n\n\n\n\n\n\nImportant\n\n\n\nThis is going to be where my analysis is.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Unpacking Urban Data in Kathmandu",
    "section": "",
    "text": "This is the project website.\n\n\n\n\n\n\nImportant\n\n\n\nThis is going to be where my analysis is.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/1.html",
    "href": "analysis/1.html",
    "title": "Visualizing Census Data",
    "section": "",
    "text": "Statistically speaking, Nepal is a poor country. It ranks 180/222 in terms of GDP per capita 1, ranking well behind other post-conflict nations of Kosovo(132) and Laos (153). 76.2% of the adult population is literate 2 and 41.9% of households sleep under roofs of galvanized tin sheets 3.\nAlmost 42% of the Nation’s populations lives in housing made out of galvanized steel. Source: 2021 Nepal Census\nHowever, such reality seems extremely far away in the capital city of Kathmandu where Teslas frequent the roads. Kathmandu’s prosperity isn’t new and stems from a list of historic and geographic factors. Situated at 4.6K ft above sea level, the Kathmandu valley is a large swath of fertile flatland in Himalayan Range where steep topography made agriculture very difficult4. Due to this geographic advantange, the region has been continously settled for at least 2000 years 5. The large settlements attracted more people and soon Kathmandu found itself as an important trading hub, linking the India-Tibet land route 6. Kings built roads, waterworks, and schools; merchants traded gold, spices and silk; while pilgrims spread art, region and philosophy. Consequentialy, Kathmandu and its residents grew rich while its surrounding countryside languished into subsistence farming.\nWe can see Kathmandu’s regional dominance displayed clearly in its 2021 Census data7. A comprehensive decinnial survey, the 2021 census is the most recent and most representative data on Nepal’s socioeconomic factors. Key metrics such as number of households and their associated wealth quintile is present at the municipal ward level in excel format. While the data is not spatial, it is possible to use data cleaning methods to convert them. This report hence marks the first spatial exploration of the Nepal census data.",
    "crumbs": [
      "Analysis",
      "Visualizing Census Data"
    ]
  },
  {
    "objectID": "analysis/1.html#wrangling-the-data",
    "href": "analysis/1.html#wrangling-the-data",
    "title": "Visualizing Census Data",
    "section": "Wrangling The Data",
    "text": "Wrangling The Data\nTo begin my analysis, I first load the ward level shapefiles for Kathmandu valley into my environment. No public GIS data exists on political boundaries—however, in its absence, civillians have taken up charge on digitizing hence. All spatial files in this analysis is thanks to the hard work of Kiran Joshi, whose blog can be found here.\nWhat is coloquially known as “Kathmandu” is in reality three cities. Legend says that three sons divided their father’s kingdom into three parts to rule equally, creating the three cities of Kathmandu, Lalitpur, and Bhaktapur. But in spirit and in function, these cities work as one. Hence, my analysis will start by calling and combining the three shapefiles.\n\nimport geopandas as gpd\nimport pandas as pd\n\nktm = gpd.read_file('../data/sfs/Kathmandu/Kathmandu.shp')\nbkt = gpd.read_file('../data/sfs/Bhaktapur/Bhaktapur.shp')\nltp = gpd.read_file('../data/sfs/Lalitpur/Lalitpur.shp')\n\ncities = pd.concat([ktm, bkt, ltp])\n\n#cleaning up the center columnm as it contains neighbourhood name information\ncities['Center'] = cities['Center'].str.replace(' Office$', '', regex=True)\n\ncities.head()\n\n\n\n\n\n\n\n\nOBJECTID\nState\nState_Code\nDistrict\nProtected\nMun_Name\nMun_Type\nCenter\nArea_SQKM\nWard_No\ngeometry\n\n\n\n\n0\n3707\nBagmati\n3\nKATHMANDU\nNaN\nGokarneshwor\nNagarpalika\nGokarneshwor Municipality\n7.0128\n3\nPOLYGON ((342664.728 3073511.945, 342660.154 3...\n\n\n1\n3708\nBagmati\n3\nKATHMANDU\nNaN\nGokarneshwor\nNagarpalika\nGokarneshwor Municipality\n4.6919\n4\nPOLYGON ((342803.560 3069107.750, 342787.801 3...\n\n\n2\n3709\nBagmati\n3\nKATHMANDU\nNaN\nGokarneshwor\nNagarpalika\nGokarneshwor Municipality\n1.0760\n5\nPOLYGON ((340158.801 3066905.027, 340126.624 3...\n\n\n3\n3710\nBagmati\n3\nKATHMANDU\nNaN\nGokarneshwor\nNagarpalika\nGokarneshwor Municipality\n0.5591\n6\nPOLYGON ((340531.184 3067725.024, 340535.003 3...\n\n\n4\n3711\nBagmati\n3\nKATHMANDU\nNaN\nGokarneshwor\nNagarpalika\nGokarneshwor Municipality\n1.3092\n8\nPOLYGON ((340879.585 3068658.365, 340889.756 3...\n\n\n\n\n\n\n\nNext, I call and clean the Household Income dataset. After a bit of manipulation and cleaning, my final dataframe gives me ward level information on the total number of households, and the number of households that fall under each wealth quintile ranging from 1 to 5 with 1 being the most economically vulnerable households.\n\nhhwealth = pd.read_excel('../data/census/HHwealth.xlsx')\n\n#creating new names for income quartiles\ncolnnames = [\n    'prov', 'dist', 'gapa', 'name', 'ward', 'tot_hh',\n    'quart1_count', 'quart2_count', 'quart3_count', 'quart4_count', 'quart5_count',\n    'quart1_%', 'quart2_%', 'quart3_%', 'quart4_%', 'quart5_%'\n]\n\n#dropping unnecesary values\nhhwealth = hhwealth.iloc[3:].reset_index(drop=True)\nhhwealth.columns = colnnames\n\n#dataset contains random text values interrupting wards, hence removing such rows\nnumeric_columns = [col for col in hhwealth.columns if col != 'name']\nfor col in numeric_columns:\n    hhwealth[col] = pd.to_numeric(hhwealth[col], errors='coerce')\n\n#only selecting the observations for the three municipal districts\nvalhhwealth = hhwealth[hhwealth['dist'].isin([28, 29, 30])]\n\nvalhhwealth.head()\n\n\n\n\n\n\n\n\nprov\ndist\ngapa\nname\nward\ntot_hh\nquart1_count\nquart2_count\nquart3_count\nquart4_count\nquart5_count\nquart1_%\nquart2_%\nquart3_%\nquart4_%\nquart5_%\n\n\n\n\n453\n3\n28\n0\nKathmandu\nNaN\n542892\n2038\n11398\n39406\n175348\n314702\n0.38\n2.10\n7.26\n32.30\n57.97\n\n\n454\n3\n28\n1\nShankharapur Municipality\nNaN\n7140\n376\n874\n1676\n2129\n2085\n5.27\n12.24\n23.47\n29.82\n29.20\n\n\n455\n3\n28\n1\nShankharapur Municipality\n1.0\n867\n83\n106\n334\n255\n89\n9.57\n12.23\n38.52\n29.41\n10.27\n\n\n456\n3\n28\n1\nShankharapur Municipality\n2.0\n426\n65\n98\n161\n69\n33\n15.26\n23.00\n37.79\n16.20\n7.75\n\n\n457\n3\n28\n1\nShankharapur Municipality\n3.0\n589\n36\n92\n156\n174\n131\n6.11\n15.62\n26.49\n29.54\n22.24\n\n\n\n\n\n\n\nAs there are no spatial geometries present in the municipal data, we would want to join the census infomation to our ward-level map by the name of the municipality and their corresponding ward. But, having been phonetically converted to English, the municipalities are not spelled the same way across the dataset. Additionally, the data also contains unnecesary (and unstandardized) descriptors which will inhibit a join.\nTo address these issues, I remove all unneeded suffixes and match my datasets using a fuzzy criteria. Two neighbourhoods are considered the same if their names are up to 80% similar.\n\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\n#helper function to clean unnecesary descriptors that don't add to the analysis\ndef clean_name(name):\n    name = name.strip()\n    suffs = [\n        ' Municipality',\n        ' Metropolitian City', \n        ' Metropolitan City',\n        ' Mahanagarpalika',\n        ' Nagarpalika',\n        ' Nagarpalika VDC',\n        ' Mun',\n        ' Gabisa Karyalaya',\n        ' Gabisa Bhawan',\n        ' Gaunpalika',\n        ' Sub Metro'\n    ]\n    clean = name\n    for suffix in suffs:\n        if clean.endswith(suffix):\n            clean = clean[:-len(suffix)].strip()\n    \n    return clean\n\n# Cleaning the column containing municipality information\ncities['Municipality'] = cities['Mun_Name'].apply(clean_name)\nvalhhwealth['Municipality'] = valhhwealth['name'].apply(clean_name)\n\n# fuzzy matching accoridng to cleaned names\ndef find_match(name, choices, min_score=80):\n    best_match = process.extractOne(name, choices)\n    if best_match and best_match[1] &gt;= min_score:\n        return best_match[0]\n    return None\n\n# creating a mapping column of fuzzy matched names\nname_mapping = {}\nfor name in cities['Municipality'].unique():\n    match = find_match(name, valhhwealth['Municipality'].unique())\n    if match:\n        name_mapping[name] = match\n\n#adding those to the dataset\ncities['new_name'] = cities['Municipality'].map(name_mapping)\n\n# joining my census data to the spatial files\ncities_inc = pd.merge(cities, \n                     valhhwealth,\n                     left_on=['new_name', 'Ward_No'],\n                     right_on=['Municipality', 'ward'],\n                     how='left')\n\n#finally, converting my data to WGS84\ncities_inc = cities_inc.to_crs('EPSG:4326')\n\nOnce the data has been joined, we can see Kathmandu’s settlement patterns immediately by observing the distribution of houses. Here, wards near the center of the city have significantly higher number of households than wards comprising the surorunding hills. The large variance in values presents problem with scaling the data making visual observation difficult.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.set_facecolor('ivory')\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Futura']\n\nplot = cities_inc.plot(ax=ax, \n    column='tot_hh',\n    cmap='BuPu',\n    edgecolor='black',\n    linewidth=0.5,\n    legend=True,\n    legend_kwds={'label': 'Total Households'}\n)\n\nax.set_title(\"Kathmandu's Population Concentrated at the Center\", \n    size=18, \n    pad=20\n)\nax.set_axis_off()\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nOne way to avoid the issue of large variance is by scaling the total number of households. The map below shows us the distribution of households (log scale) in Kathmandu where darker coloured wards corresponds to a higher concentration of houses. We are able to see the trend of increasing concentration near the city center much clearer in this map, though the issue of restricting city limits still remains.\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.set_facecolor('ivory')\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Futura']\n\ncities_inc['log_hh'] = np.log(cities_inc['tot_hh'])\nplot = cities_inc.plot(ax=ax, \n    column='log_hh',\n    cmap='BuPu',\n    edgecolor='black',\n    linewidth=0.5,\n    legend=True,\n    legend_kwds={'label': 'Log Total Households'}\n)\n\nax.set_title(\"Kathmandu's Population Concentrated at the Center (Log Scale)\", \n    size=18, \n    pad=20\n)\nax.set_axis_off()\n\nplt.tight_layout()",
    "crumbs": [
      "Analysis",
      "Visualizing Census Data"
    ]
  },
  {
    "objectID": "analysis/1.html#setting-limits-for-kathmandu",
    "href": "analysis/1.html#setting-limits-for-kathmandu",
    "title": "Visualizing Census Data",
    "section": "Setting Limits for Kathmandu",
    "text": "Setting Limits for Kathmandu\nOur analysis will not be very useful if we include sparsely populated hill wards in our dataset. Our study area needs to have tigther city limits—for this purpose I use a road density based selection criteria. Using road network data from Open Street Maps, I calculate the density of streets by area for each ward. I then filter out all wards who fall in the bottom 30th percentile for road density.\n\nimport osmnx as ox\nvalley = cities_inc.dissolve().make_valid()\nstreet_network = ox.graph_from_polygon(valley.geometry.iloc[0], network_type=\"drive\")\nox.plot_graph(ox.project_graph(street_network), node_size = 0)\n\n\n\n\n\n\n\n\nAbove, the map of the street network also presents a similar story of density, where the network is more concentrated in the North-Center area of the valley. Lets remove now all the less dense wards.\n\nfrom shapely.geometry import Polygon\n\n#getting street edges\nedges = ox.graph_to_gdfs(street_network, edges = True, nodes = False).to_crs(cities_inc.crs)\n\n#counting number of streets per ward\ncities_inc['streetcount'] = cities_inc.geometry.apply(\n    lambda x: len(edges[edges.intersects(x)])  \n)\n\n#creating density metric\ncities_inc['streetdensity'] = cities_inc['streetcount']/cities_inc['Area_SQKM']\n\n#selecting 30th quartile threshold\nthreshold = cities_inc['streetdensity'].quantile(0.30)\n\n#subsetting values\nmain_city = cities_inc[cities_inc['streetdensity'] &gt;= threshold]\n\n\n#writing valley geojson\nvalley.to_file(\"../data/sfs/valley.geojson\")\n\n#writing cities geojson\nmain_city.to_file(\"../data/sfs/maincity.geojson\")\n\nThe resulting map shows a much tighter city limits though the issues with scale still presists which we will address by logging the total number of households. Here, we see conspicous pockets of empty space surrounded by a growing belt of dense settlements. There are three empty cores—each corresponding to the historic city centers of Kathmandu, Lalitpur, and Bhaktapur.\n\nfrom shapely.geometry import Point\n#  Creating a gdf of historic city center area\ncenters = gpd.GeoDataFrame(\n    {\n        'name': ['Kathmandu', 'Patan', 'Bhaktapur'],\n        'geometry': [\n            Point(85.30727341768024, 27.705040937390145),\n            Point(85.32530690762836, 27.672831546274733),\n            Point(85.42829510275436, 27.672202975996818)\n        ]\n    },\n    crs='EPSG:4326'\n)\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax.set_facecolor('ivory')\n\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Futura']\n\nmain_city['log_hh'] = np.log(main_city['tot_hh'])\n\nmain_city.plot(ax=ax, \n    column='log_hh',\n    cmap='BuPu',\n    edgecolor='black',\n    linewidth=0.5,\n    legend=True,\n    legend_kwds={'label': 'Density of Households'}\n)\n\ncenters.plot(ax=ax,\n           color=\"gold\",\n           edgecolor='darkmagenta',\n           alpha=0.3,\n           markersize=500)  # Increase dot size\n\nfor idx, row in centers.iterrows():\n   ax.annotate(row['name'], \n               xy=(row.geometry.x, row.geometry.y),\n               color='darkmagenta',\n               bbox=dict(\n                  facecolor='bisque',\n                  edgecolor='darkmagenta',\n                  alpha=0.7,\n                  pad=2\n                  ),\n               alpha=0.7,\n               xytext=(5, 5), \n               textcoords='offset points')\n\nax.set_title(\"Empty Urban Core Surrounded by Dense Belt\", \n    size=18, \n    pad=20\n)\nax.set_axis_off()\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe empty core areas on the map have long been the commerical and political center of Kathmandu. In the above map, the labeled areas corresponds to the historic districts of the three cities. These ancient city centers are a major draw for commerce and tourism, though I can see how their bustle and chaos would push away residential households.\n \n\nThe Historic Kathmandu Palace Complex and a nearby shopping square. These historic districts still contains shops and squares that have been present for centuries. A trip to Kathmandu is incomplete without a visit to at least one (but honestly all three) of these centers, though the age of some of these houses may make them uncomfortable to live in long term.\nThe settlement pattern of Kathmandu also visually corresponds to the “donut-shaped” city—declining and empty city centers surrounded by a growing residential belt. While the city centers are still important hubs of commerce, housing stock in these areas are old, cramped, and lack modern amenities. As a result, buildings in this area is increasingly converted to solely commerical units while families move to more spacious (and modern) homes in the residential belt.",
    "crumbs": [
      "Analysis",
      "Visualizing Census Data"
    ]
  },
  {
    "objectID": "analysis/1.html#where-do-the-rich-live",
    "href": "analysis/1.html#where-do-the-rich-live",
    "title": "Visualizing Census Data",
    "section": "Where do the Rich Live?",
    "text": "Where do the Rich Live?\nUrban theory suggests that people living in these residential belts are likely to be wealthier than their peers. However, spatial distribution in Kathmandu is unlikely to be uniform. Even in the residential belt, some wards are going to have a higher concentration of rich households than others. In this section, we seek to understand the distribution of households by their wealth quintiles in the city.\nFirst, we need to understand how the ward polygons correspond to the actual map of the city. Using the folium library, I created a map where users can toggle layers of highest and lowest income quartiles over the OSM layout of the city.\n\nimport folium\nm = main_city.explore(\n    column='quart5_%',\n    legend=True,\n    cmap='viridis',\n    name='Highest Income',\n    tooltip=['name', 'ward', 'quart5_%', 'quart4_%', 'quart3_%', 'quart2_%','quart1_%']\n)\nmain_city.explore(\n    column='quart1_%',\n    cmap='inferno',\n    name='Lowest Income',\n    m=m ,\n    tooltip=['name', 'ward', 'quart5_%', 'quart4_%', 'quart3_%', 'quart2_%','quart1_%']\n)\n\nfolium.LayerControl().add_to(m)\n\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFrom this map, we are able to derive the following insights:\n\nWard 5 (known locally as Lagankhel) of Lalitpur has the highest concentration of wealthy households in the city. This makes sense considering the social context of the city. This area is home to a lot of INGOs and their very well-paid staff. Additionally, out of the three cities, Lalitpur is known to be the wealthiest with an almost 100% adult literacy rate.\nIn the periphery of the city, Ward 1 of Nagarjun and Wards 1,2,and 4 of Kathmandu also show significant concentration of wealth. While the wards of Kathmandu are already known locally as being wealthy, the high concentration of wealthy households in Nagarjun stands out as that area is far to the city outskirts. However, this makes sense considering the growing suburban trend of Kathmandu.\nAreas near the historic palace complexes have the lowest incidence of high income households in the city, underscoring our observations of declining urban core.\nDistribution of lowest quantiles doesn’t tell us much due to the high variance of wealth in Nepal. Residents who are wealthy outside the valley may find themselves struggling in the city due to high living costs. Hence, lower wealth quintiles are unlikely to be represented in such scale in the city. However, we do notice that presence of households in the lowest quantile increases rapidly once we enter the hills in the western parrt of the city.\n\nTo study the distribution of household wealth, I create an interactable visualization using hvplot. Now, we can toggle between various economic quintiles and see their distribution. One key thing we notice from this new visualization is regarding our assumption about the relative prosperity of the valley. The lowest 2 quartiles are virtually unrepresented in the valley, with the concentration only increasing starting from quartile 3, or the middle income quartile.\n\nimport hvplot.pandas\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\n\nplot1 = main_city.melt(\n    id_vars=[\"OBJECTID\", \"geometry\"],\n    value_vars=['quart5_%', 'quart4_%', 'quart3_%', 'quart2_%','quart1_%'],\n    var_name=\"Quintile\",\n    value_name=\"Percentage\"\n)\n\n\nplot1.hvplot(geo=True, \n             c='Percentage', \n             alpha=0.45, \n             groupby='Quintile',\n             widget_location='left_top',\n             color_key='viridis',\n             tiles='OSM')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\nEstimating Income Distribution\nThough the census data doesn’t provide us with information on median household income, it is possible to estimate this data using other publicly available government data. The National Statistics Office has information on the median and mean income values for each quartile on their website.\nI used this information to come up with weighted estimates of median income for each ward using the proportion of quantile distribution.\n\nplot2 = main_city.copy()\n\nmed_inc = {\n    'quart1': 242797,  \n    'quart2': 299341,  \n    'quart3': 377000,\n    'quart4': 465301,  \n    'quart5': 617882  \n}\n\nplot2['Est Income'] = (\n    plot2['quart1_count'] * med_inc['quart1'] +\n    plot2['quart2_count'] * med_inc['quart2'] +\n    plot2['quart3_count'] * med_inc['quart3'] +\n    plot2['quart4_count'] * med_inc['quart4'] +\n    plot2['quart5_count'] * med_inc['quart5']\n) / plot2[['quart1_count', 'quart2_count', 'quart3_count', \n        'quart4_count', 'quart5_count']].sum(axis=1)\n\n\nplot2['Est Income'] = pd.to_numeric((plot2['Est Income'])/100000).round(1)\n\nPlotting the results gives me the following graph showing the distribution of estimated annual income. Here, income is given in 100k NRP per year, roughly equivalent to $735 USD. From the map, we can see two rings of high income wards surrounding the empty urban cores of Kathmandu and Lalitpur. Key feature to note is the double layer of residential settlements with a high income ring just surrounding the empty urban core, and an middle income ring wrapping at the very outskirts. Once, again, this typology corresponds to the Chicago Concentric Ring Theory of Urban Planning.\n\nimport altair as alt\n\nalt.Chart(plot2).mark_geoshape(\n    stroke='white',\n    strokeWidth=0.5\n).encode(\n    color=alt.Color('Est Income:Q', \n        scale=alt.Scale(scheme='plasma'),\n        title='Annual Income (Lakhs NRP)'),\n    tooltip=['Mun_Name:N', 'ward:N','Est Income:Q']\n).properties(\n    width=800,\n    height=600,\n    title='Two Rings of High Income Settlements in the City'\n)",
    "crumbs": [
      "Analysis",
      "Visualizing Census Data"
    ]
  },
  {
    "objectID": "analysis/1.html#where-do-the-poor-live",
    "href": "analysis/1.html#where-do-the-poor-live",
    "title": "Visualizing Census Data",
    "section": "Where do the Poor Live?",
    "text": "Where do the Poor Live?\nKathmandu is a mixed-income city where the rich live alongside (sometimes as stay-in landlords of) the poor. However, this does’t mean that every ward in the city is made up of the same composition of households. Some will be more well off compared to others.\nHowever, the information provided by the census doesn’t help much to mapping poverty in the context of Kathmandu. To address the issues outilned above, I combined the proportion of the lowest 3 quartiles to make a single “Low Income” catagory fo the city.\n\nplot3 = main_city.copy()\nplot3['Low Income'] = plot3[['quart1_%', 'quart2_%', 'quart3_%']].sum(axis=1)\n\nfig, ax = plt.subplots(figsize=(15, 10), facecolor='ivory')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.sans-serif'] = ['Futura']\n\nplot3.plot(ax=ax, column='Low Income', \n          cmap='RdPu',\n          edgecolor='darkseagreen',\n          linewidth=0.5,\n          legend=True,\n          legend_kwds={'label': '% of Households in Low Income Quartile',\n                      'orientation': 'vertical'}) \n\n\n\ncenters.plot(ax=ax,\n           color=\"gold\",\n           edgecolor='darkmagenta',\n           alpha=0.3,\n           markersize=500) \n\nfor idx, row in centers.iterrows():\n   ax.annotate(row['name'], \n               xy=(row.geometry.x, row.geometry.y),\n               color='darkmagenta',\n               bbox=dict(\n                  facecolor='bisque',\n                  edgecolor='darkmagenta',\n                  alpha=0.7,\n                  pad=2\n                  ),\n               alpha=0.7,\n               xytext=(15, 5), \n               textcoords='offset points')\n\nax.set_title(\"Concentration of Poverty in Historic Cores\", \n    size=18, \n    pad=20\n)\nax.set_axis_off()\n\n\n\n\n\n\n\n\nUnsurprisingly, the distribution of low income households is the inverse of the income distribution map. Here, the center and the periphery shows a higher concentration of low income households.",
    "crumbs": [
      "Analysis",
      "Visualizing Census Data"
    ]
  },
  {
    "objectID": "analysis/2.html",
    "href": "analysis/2.html",
    "title": "Worst Places to Live in Kathmandu",
    "section": "",
    "text": "Despite Kathmandu’s natural and historic beauty, the city in recent years has seen a rapid decline in living conditions. Haphazard sprawl, mismanaged public transport, rapid loss of green space, and climate change have been affecting the liveability of the city 1. While everyone acknowledges this problem, there is still a lack of quantified research into this topic, especially from an urban planning perspective. This is an issue for the city that contains 24% of the country’s urban population, and contributes to 33% of the country’s GDP 2. With agricultural and forested land increasingly being decimated to accomodate this sprawl, the valley is facing an environmental crisis that routinely puts it as the most polluted city in the world 3.\nI seek to examine these trends, especially as they relate to loss of greenery and urban heat islands in this section of the report. Using satellite data from Landsat 8, I seek to calculate the NDVI (Normalized Difference Vegetation Index), a measure for greenergy, as well as the Land Surface Temperature (LST), a measure for temperature, for the city between the years 2013 and 2023. Using the insights gathered from this extraction, I seek to identify the regions of the city that perform worse on these indexes. Combining the output from my raster data to the census data in part 1, I seek to put forth a novel, intersectional analysis of sprawl, wealth and environmental degredation in the city.",
    "crumbs": [
      "Analysis",
      "Worst Places to Live in Kathmandu"
    ]
  },
  {
    "objectID": "analysis/2.html#getting-landsat-data-with-earth-engine",
    "href": "analysis/2.html#getting-landsat-data-with-earth-engine",
    "title": "Worst Places to Live in Kathmandu",
    "section": "Getting Landsat Data with Earth Engine",
    "text": "Getting Landsat Data with Earth Engine\nTo begin my analysis, I first download and extract my required files from the Google Earth Engine API. I will first call an earth engine object that contains composite, cloud cover minimized values for the years 2013 and 2023. Then, using the valley and city boundaries extracted in part 1 of my analysis, I will limit these shapefiles to the boundaries of my study_area and perform the analysis.\n\n#setting up GEE\nimport ee\nee.Authenticate()\nee.Initialize()\n\n\nimport geopandas as gpd\n#calling valley geometry \nvalley = gpd.read_file(\"../data/sfs/valley.geojson\")\n#creating an earth engine object of the boundary \nee_geometry = ee.Geometry(valley.__geo_interface__['features'][0]['geometry'])\n\nI have already created my composite, saved it to my google drive, and downloaded it locally on my repository. The code section below details my approach. For my analysis, I will begin by calling in my locally saved files in the next section.\n\n# setting up my start and end dates for two years\nstart_date_2013 = '2013-01-01'\nstart_date_2023 = '2023-01-01'\nend_date_2013 = '2013-12-31'\nend_date_2023 = '2023-12-31'\n\n#creating yearly composite\ncomp_2013 =  ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filterBounds(ee_geometry).filterDate(start_date_2013, end_date_2013).filter(ee.Filter.lt('CLOUD_COVER', 20)).median()\ncomp_2023 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')\\\n    .filterBounds(ee_geometry)\\\n    .filterDate(start_date_2023, end_date_2023)\\\n    .filter(ee.Filter.lt('CLOUD_COVER', 20))\\\n    .median()\n\n\n\n#Creating tasks to extract it from Earth engine to my local google drive\ntask_2013 = ee.batch.Export.image.toDrive(\n    image=comp_2013,\n    description='landsat_13',\n    scale=30, \n    region=ee_geometry,\n    maxPixels=1e9,\n    fileFormat='GeoTIFF'\n)\n\n# Export 2023 composite\ntask_2023 = ee.batch.Export.image.toDrive(\n    image=comp_2023,\n    description='landsat_23',\n    scale=30,\n    region=ee_geometry,\n    maxPixels=1e9,\n    fileFormat='GeoTIFF'\n)\n\n#starting tasks\ntask_2013.start()\ntask_2023.start()",
    "crumbs": [
      "Analysis",
      "Worst Places to Live in Kathmandu"
    ]
  },
  {
    "objectID": "analysis/2.html#calculating-ndvi-and-lst",
    "href": "analysis/2.html#calculating-ndvi-and-lst",
    "title": "Worst Places to Live in Kathmandu",
    "section": "Calculating NDVI and LST",
    "text": "Calculating NDVI and LST\nI have the data already loaded to my local system, so I will start by loading it in. I will also mask my rasters to the extent of my study area. After loading the data, two key environmental measures will be extracted—NDVI, which measures the amount of greenery, and LST values of the extent.\nNDVI is calculated using Landsat’s Near-Infrared (Band 5) and Red (Band 4) bands, following the formula: NDVI = (NIR - Red)/(NIR + Red), where NIR is Band 5 and Red is Band 4. This index ranges from -1 to 1, with higher values indicating denser vegetation.\n\nimport rasterio as rio\nfrom shapely.geometry import box\nfrom rasterio.mask import mask\nimport numpy as np\n\n#calling my rasters\nlandsat13 = rio.open(\"../data/landsat/landsat_13.tif\")\nlandsat23 = rio.open(\"../data/landsat/landsat_23.tif\")\n\n#calling my city file\nmain_city = gpd.read_file('../data/sfs/maincity.geojson')\n\n\nstudy_masked13, study_transform13 = mask(\n    dataset=landsat13,              \n    shapes=main_city.geometry,  \n    crop=True,                    \n    all_touched=True,            \n    filled=False,                 \n)\n\nstudy_masked23, study_transform23 = mask(\n    dataset=landsat23,              \n    shapes=main_city.geometry,  \n    crop=True,                    \n    all_touched=True,            \n    filled=False,                 \n)\n\n#getting the needed layers\nred13 = study_masked13[4]\nnir13 = study_masked13[5]\nred23 = study_masked23[4]\nnir23= study_masked23[5]\n\n#calculating NDVI\ndef calculate_NDVI(nir, red):\n    \"\"\"\n    Calculate the NDVI from the NIR and red landsat bands\n    \"\"\"\n\n    # Convert to floats\n    nir = nir.astype(float)\n    red = red.astype(float)\n\n    # Get valid entries\n    check = np.logical_and(red.mask == False, nir.mask == False)\n\n    # Where the check is True, return the NDVI, else return NaN\n    ndvi = np.where(check, (nir - red) / (nir + red), np.nan)\n    \n    # Return\n    return ndvi\n\nNDVI13 = calculate_NDVI(nir13, red13)\nNDVI23 = calculate_NDVI(nir23, red23)\n\nLST can be calculated using Landsat’s Thermal Infrared (Band 10) data through a multi-step process: first converting the digital numbers to top-of-atmosphere radiance, then to brightness temperature, and finally applying emissivity corrections to obtain land surface temperature in Kelvin or Celsius. The LST calculations requires a bit of physics that is beyond me, but I got the steps from this article.\n\ntherm13 = study_masked13[10].astype(float)\ntherm23 = study_masked23[10].astype(float)\n\ndef calculate_lst(thermal, ndvi_data):\n    \"\"\"Calculate Land Surface Temperature using thermal and NDVI data\"\"\"\n    # Constants\n    k1 = 774.8853  # First thermal constant for Landsat 8 Band 10\n    k2 = 1321.0789 # Second thermal constant for Landsat 8 Band 10\n    \n    radiance = thermal * 0.00341802 + 149\n    \n    # Check valid (unmasked) pixels\n    check = np.logical_and(~np.isnan(radiance), ~np.isnan(ndvi_data))\n\n    bt = k2 / (np.log((k1 / radiance) + 1))\n    \n    ndvi_min = np.nanmin(ndvi_data[~np.isnan(ndvi_data)])  \n    ndvi_max = np.nanmax(ndvi_data[~np.isnan(ndvi_data)])  \n    \n    p_array = np.where(check, \n                      ((ndvi_data - ndvi_min)/(ndvi_max - ndvi_min))**2,\n                      np.nan)\n    \n    emissivity = 0.004 * p_array + 0.986\n    \n    lst = bt/(1 + (0.00115 * bt/1.4388) * np.log(emissivity))\n    \n    return lst - 273.5\n\nlst13sad = calculate_lst(therm13, NDVI13)\n\nHowever, no matter how hard I tried, I couldn’t get this data to work out for me. It kept giving me wrong values and after a time, I decided to call my losses and skip this section. If I had more time then I would do the LST calcualtions as well.\n\nimport matplotlib.pyplot as plt\nim = plt.imshow(lst13sad, cmap='RdBu_r')\nplt.title('PLotting my failiure of LST 13 :( )')\n\ncbar = plt.colorbar(im)\ncbar.set_label('LST')\n\n# Hide axes for cleaner look\nplt.axis('off')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPlotting NDVI\nFirst, let us plot the greenery in the valley in 2013 and 2024. NDVI assigns a normalized value between -1 to 1 depending on the availability of vegetation in an area.\n\n# Create figure with two subplots side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Plot 2008 NDVI\nim1 = ax1.imshow(NDVI13, cmap='RdYlGn', vmin=-1, vmax=1)\nax1.set_title('NDVI 2013')\nax1.axis('off')  # Hide axes for cleaner look\n\n# Plot 2023 NDVI\nim2 = ax2.imshow(NDVI23, cmap='RdYlGn', vmin=-1, vmax=1)\nax2.set_title('NDVI 2023')\nax2.axis('off')  # Hide axes for cleaner look\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nWe can see the difference between the two decades and find the areas which has experienced most loss of greenery.\n\nNDVI_diff = NDVI23 - NDVI13\n\nim = plt.imshow(NDVI_diff, cmap='RdYlGn')\nplt.title('NDVI Change (2023 - 2013)')\n\n# Add colorbar\ncbar = plt.colorbar(im)\ncbar.set_label('NDVI Difference')\n\n# Hide axes for cleaner look\nplt.axis('off')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLets see how that transforms to ward data.\n\nfrom rasterstats import zonal_stats\n\nNDVI_diff= np.where(np.isnan(NDVI_diff), -9999, NDVI_diff)\nstats = zonal_stats(\n    main_city,  \n    NDVI_diff,\n    affine=landsat23.transform,  \n    stats=['min', 'max', 'mean', 'median', 'majority'],\n    nodata=-9999,\n)\n\nmean_stats = [stats_dict[\"mean\"] for stats_dict in stats]\nmain_city[\"Avg NDVI Change\"] = mean_stats\n\n\nplt.figure(figsize=(10, 10))\nmain_city.plot(column='Avg NDVI Change', cmap='RdYlGn', legend=True)\nplt.title('Average NDVI Change per Ward')\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1000x1000 with 0 Axes&gt;",
    "crumbs": [
      "Analysis",
      "Worst Places to Live in Kathmandu"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Avani Adhikari, a student for this python class. Here is a picture from my favourite game, Hollow Knight.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "data/sfs/Lalitpur/Lalitpur.html",
    "href": "data/sfs/Lalitpur/Lalitpur.html",
    "title": "MUSA 550",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  All_Merged_Districts_Ward_Wi2  ENG dataset\n\nAll_Merged_Districts_Ward_Wi2\n\n                 0 0     false"
  }
]